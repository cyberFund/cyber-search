---

# Image settings https://github.com/docker-library/docs/tree/master/cassandra

# CASSANDRA_SEEDS -> specify ~3 seed nodes for dc.
# They are used by gossip for bootstrapping new nodes joining a cluster.

# Ports:
#    7199: cassandra JMX
#    1099  kafka jmx
#    9042: CQL
#    9200: ElasticSearch HTTP
#    2181: zookeper
#    9092: kafka
#
#    8000: kafka-topics-ui
#    9000: kafka-manager
# todo find a way to not expose as host due mac team members

version: '3.3'
services:

  zoo:
    network_mode: host
    image: confluentinc/cp-zookeeper:3.3.0
    environment:
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_CLIENT_PORT: 2181
      KAFKA_LOG4J_ROOT_LOGLEVEL: ERROR
    volumes:
      - /opt/cyberfund/search/zoo/data:/var/lib/zookeeper/data
      - /opt/cyberfund/search/zoo/log:/var/lib/zookeeper/log


  kafka:
    network_mode: host
    image: confluentinc/cp-kafka:3.3.0
    environment:
      KAFKA_BROKER_ID: 0
      KAFKA_ZOOKEEPER_CONNECT: localhost:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 1099
      KAFKA_LOG4J_ROOT_LOGLEVEL: ERROR
      KAFKA_MESSAGE_MAX_BYTES: 10485760
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760
    volumes:
      - /opt/cyberfund/search/kafka/data:/var/lib/kafka/data


  kafka_manager:
    network_mode: host
    image: hlebalbau/kafka-manager:1.3.3.13
    environment:
      ZK_HOSTS: "localhost:2181"
      APPLICATION_SECRET: "random-secret"
    command: -Dpidfile.path=/dev/null


  kafka-connect:
    network_mode: host
    image: confluentinc/cp-kafka-connect:3.3.0
    environment:

      CONNECT_REST_ADVERTISED_HOST_NAME: "localhost"
      CONNECT_REST_ADVERTISED_PORT: "8083"
      CONNECT_REST_PORT: 8083


      CONNECT_PLUGIN_PATH: /etc/kafka-connect/custom-plugins

      CONNECT_BOOTSTRAP_SERVERS: localhost:9092
      CONNECT_ZOOKEEPER_CONNECT: localhost:2181

      CONNECT_GROUP_ID: "cybernode"
      CONNECT_CONFIG_STORAGE_TOPIC: "cybernode.config"
      CONNECT_OFFSET_STORAGE_TOPIC: "cybernode.offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "cybernode.status"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 5000

      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"

      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE: "true"
      CONNECT_INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE: "true"

      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      CONNECT_CONSUMER_MAX_PARTITION_FETCH_BYTES: 10485760
      CONNECT_PRODUCER_MAX_REQUEST_SIZE: 10485760

      CONNECT_ACCESS_CONTROL_ALLOW_ORIGIN: '*'
      CONNECT_ACCESS_CONTROL_ALLOW_METHODS: 'GET,OPTIONS,HEAD,POST,PUT,DELETE'
    volumes:
      - ../cyber-connectors/:/etc/kafka-connect/custom-plugins



  kafka-rest:
    network_mode: host
    image: confluentinc/cp-kafka-rest:3.3.0
    environment:
      KAFKA_REST_ZOOKEEPER_CONNECT: localhost:2181
      KAFKA_REST_LISTENERS: http://localhost:8082
      KAFKA_REST_HOST_NAME: localhost
      KAFKA_REST_ACCESS_CONTROL_ALLOW_ORIGIN: '*'
      KAFKA_REST_ACCESS_CONTROL_ALLOW_METHODS: 'GET,OPTIONS,HEAD,POST,PUT,DELETE'



  fast-data-dev:
    network_mode: host
    image: landoop/fast-data-dev:cp3.3.0
    environment:
       WEB_ONLY: 1
       RUNTESTS: 0